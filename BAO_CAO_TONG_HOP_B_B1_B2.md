# BÃO CÃO Tá»”NG Há»¢P: SO SÃNH PHÆ¯Æ NG ÃN B, B1, B2

## ğŸ“Š Tá»”NG QUAN

BÃ¡o cÃ¡o nÃ y tá»•ng há»£p káº¿t quáº£ thá»±c nghiá»‡m cá»§a 3 phÆ°Æ¡ng Ã¡n dá»± Ä‘oÃ¡n **Táº£i tiÃªu thá»¥ bÄƒng táº£i** dá»±a trÃªn **4 biáº¿n Ä‘á»™c láº­p thá»±c táº¿**: `Ä‘á»™ áº©m`, `nhiá»‡t Ä‘á»™`, `gÃ³c nghiÃªng`, `ca` (khÃ´ng sá»­ dá»¥ng cÃ¡c biáº¿n phÃ¡i sinh cÃ³ data leakage).

| PhÆ°Æ¡ng Ã¡n | Má»¥c tiÃªu | PhÆ°Æ¡ng phÃ¡p chÃ­nh | Dá»¯ liá»‡u |
|-----------|----------|-------------------|---------|
| **B** | Baseline | MLP + PolynomialFeatures(2) | 5,220 samples gá»‘c |
| **B1** | Cáº£i tiáº¿n B | Optuna + Ensemble + Stacking | 5,220 samples gá»‘c |
| **B2** | Äá»™t phÃ¡ | Deep Learning + Data Augmentation | 25,000 samples (augmented) |

---

## ğŸ† Káº¾T QUáº¢ Tá»”NG Há»¢P

### Báº£ng So SÃ¡nh Top Model

| Chá»‰ sá»‘ | **PhÆ°Æ¡ng Ãn B** | **PhÆ°Æ¡ng Ãn B1** | **PhÆ°Æ¡ng Ãn B2** | **Cáº£i thiá»‡n tá»‘t nháº¥t** |
|--------|----------------|-----------------|-----------------|----------------------|
| **Best Model** | MLP_(32) | Baseline_B (MLP_32_Poly2) | **PT_CNN1D (Augmented)** | B2 |
| **MAE** | 286.92 | 286.92 | **282.93** â­ | **-3.99** (-1.39%) |
| **MAPE (%)** | 2.698 | 2.698 | **2.655** â­ | **-0.043** (-1.59%) |
| **RÂ²** | 0.9276 | 0.9276 | **0.9290** â­ | **+0.0014** |
| **RMSE** | 332.14 | 330.37 | **328.88** â­ | **-3.26** |
| **Sá»‘ mÃ´ hÃ¬nh thá»­** | 27 | 12 | 17 | - |
| **Data size** | 5,220 | 5,220 | **25,000** | - |
| **Thá»i gian train** | Trung bÃ¬nh | Cao (Optuna 200 trials) | Ráº¥t cao (GPU) | - |

### Xáº¿p háº¡ng hiá»‡u suáº¥t

```
ğŸ¥‡ PhÆ°Æ¡ng Ãn B2:  MAE = 282.93 | MAPE = 2.655% | RÂ² = 0.9290
ğŸ¥ˆ PhÆ°Æ¡ng Ãn B:   MAE = 286.92 | MAPE = 2.698% | RÂ² = 0.9276
ğŸ¥‰ PhÆ°Æ¡ng Ãn B1:  MAE = 286.92 | MAPE = 2.698% | RÂ² = 0.9276
```

**Káº¿t luáº­n nhanh:** `B2 > B = B1`

---

## ğŸ“ˆ PHÃ‚N TÃCH CHI TIáº¾T Tá»ªNG PHÆ¯Æ NG ÃN

### 1. PHÆ¯Æ NG ÃN B â€” Baseline Vá»¯ng Cháº¯c

#### Káº¿t quáº£

- **Best Model:** MLP_(32) â€” Máº¡ng neural 1 lá»›p áº©n 32 neurons
- **Features:** PolynomialFeatures(degree=2) â†’ 14 features
- **Hiá»‡u suáº¥t:** MAE = 286.92 | MAPE = 2.698% | RÂ² = 0.9276

#### Top 5 Models

| Rank | Model | MAE | MAPE (%) | RÂ² | Train Time (s) |
|------|-------|-----|----------|-----|----------------|
| 1 | MLP_(32) | 286.92 | 2.698 | 0.9276 | 9.61 |
| 2 | Ridge | 287.58 | 2.704 | 0.9286 | 0.01 |
| 3 | Stacking(MLP+Ridge+MLP) | 287.72 | 2.705 | 0.9285 | 59.57 |
| 4 | XGB_Optimized | 287.84 | 2.704 | 0.9277 | 112.93 |
| 5 | LGBM_Optimized | 287.87 | 2.706 | 0.9280 | 96.88 |

#### Nháº­n xÃ©t

âœ… **Æ¯u Ä‘iá»ƒm:**
- ÄÆ¡n giáº£n, nhanh, hiá»‡u quáº£
- MLP_(32) vá»›i RandomizedSearchCV Ä‘Ã£ cho káº¿t quáº£ tá»‘t nháº¥t
- Ridge regression Ä‘á»©ng thá»© 2 vá»›i thá»i gian train cá»±c nhanh (0.01s)
- CÃ¡c mÃ´ hÃ¬nh tree-based (XGB, LGBM, RF, GB) Ä‘á»u á»•n Ä‘á»‹nh á»Ÿ MAE ~288

âŒ **Háº¡n cháº¿:**
- 25/27 mÃ´ hÃ¬nh náº±m trong khoáº£ng MAE 286-290 â†’ **ceiling rÃµ rÃ ng**
- Stacking khÃ´ng cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ so vá»›i single model
- Má»™t sá»‘ mÃ´ hÃ¬nh (MLP_(4), MLP_tanh) phÃ¢n ká»³ nghiÃªm trá»ng

ğŸ”¬ **Insight:**
- **Polynomial degree 2 lÃ  Ä‘á»§** â€” táº¡o Ä‘á»§ interaction features mÃ  khÃ´ng overfitting
- **MLP Ä‘Æ¡n giáº£n > MLP phá»©c táº¡p**: MLP_(32) tá»‘t hÆ¡n MLP_(64), MLP_(64_32_16)
- Cho tháº¥y **báº£n cháº¥t cá»§a bÃ i toÃ¡n khÃ´ng phá»©c táº¡p**, MLP nhá» lÃ  Ä‘á»§

---

### 2. PHÆ¯Æ NG ÃN B1 â€” Tá»‘i Æ¯u ChuyÃªn SÃ¢u (NhÆ°ng KhÃ´ng Äá»™t PhÃ¡)

#### Káº¿t quáº£

- **Best Model:** Baseline_B (MLP_32_Poly2) â€” giá»‘ng há»‡t PhÆ°Æ¡ng Ãn B
- **Hiá»‡u suáº¥t:** MAE = 286.92 | MAPE = 2.698% | RÂ² = 0.9276
- **Cáº£i thiá»‡n so vá»›i B:** **0.00%** âš ï¸

#### Top 5 Models

| Rank | Model | MAE | MAPE (%) | RÂ² | Note |
|------|-------|-----|----------|-----|------|
| 1 | Baseline_B (MLP_32_Poly2) | 286.92 | 2.698 | 0.9276 | Giá»‘ng PhÆ°Æ¡ng Ãn B |
| 2 | Stacking_V1 (->Ridge) | 287.20 | 2.702 | 0.9288 | MLP+XGB+LGBM â†’ Ridge meta |
| 3 | Stacking_V2 (->Ridge) | 287.20 | 2.702 | 0.9288 | 5 models â†’ Ridge meta |
| 4 | MLP_MultiSeed | 287.29 | 2.702 | 0.9286 | 10 seeds average |
| 5 | Mega_Ensemble_3x10 | 287.52 | 2.703 | 0.9284 | 30 models ensemble |

#### CÃ¡c ká»¹ thuáº­t Ä‘Ã£ Ã¡p dá»¥ng

1. **Phase 1: Feature Engineering**
   - Thá»­ 5 feature sets: Poly2, Poly3, Poly2+Manual, Poly3+Manual, Manual
   - Thá»­ 3 scalers: StandardScaler, MinMaxScaler, RobustScaler
   - **Káº¿t luáº­n:** Poly2 (14 features) + StandardScaler lÃ  tá»‘t nháº¥t
   
2. **Phase 2: Optuna Bayesian Optimization**
   - MLP_Optuna: 200 trials â†’ architecture (26, 71, 38, 119), MAE_CV=279.29
   - XGB_Optuna: 200 trials â†’ 574 estimators, max_depth=3, MAE_CV=279.97
   - LGBM_Optuna: 200 trials â†’ 1242 estimators, max_depth=3, MAE_CV=279.45
   
3. **Phase 3: Multi-seed Ensemble**
   - MLP: 10 seeds â†’ MAE=287.29
   - XGB: 10 seeds â†’ MAE=288.02
   - LGBM: 10 seeds â†’ MAE=287.57
   
4. **Phase 4: Stacking & Voting**
   - Stacking V1, V2, V3 vá»›i meta-learners khÃ¡c nhau
   - Voting ensemble

#### Nháº­n xáº¿t

âœ… **GiÃ¡ trá»‹ khoa há»c:**
- **XÃ¡c nháº­n ceiling**: Vá»›i 4 biáº¿n gá»‘c + Poly2, ML truyá»n thá»‘ng Ä‘Ã£ Ä‘áº¡t tráº§n ~2.7% MAPE
- **Feature engineering khoa há»c**: 15 bá»™ (feature set Ã— scaler) Ä‘Æ°á»£c thá»­ â†’ Poly2 + Standard lÃ  tá»‘t nháº¥t
- **Optuna hiá»‡u quáº£ cho CV**: MAE_CV giáº£m xuá»‘ng ~279, nhÆ°ng test set váº«n ~287 â†’ risk of overfitting CV

âŒ **Háº¡n cháº¿:**
- **KhÃ´ng cáº£i thiá»‡n test performance** â€” táº¥t cáº£ models Ä‘á»u MAE â‰¥ 286.92
- Ensemble phá»©c táº¡p khÃ´ng mang láº¡i lá»£i Ã­ch
- Thá»i gian tÃ­nh toÃ¡n cao (Optuna 200 trials Ã— 3 models)

ğŸ”¬ **Insight quan trá»ng:**
- **Data leakage lÃ  khÃ´ng thá»ƒ bÃ¹ Ä‘áº¯p báº±ng ká»¹ thuáº­t**: PhÆ°Æ¡ng Ãn A (cÃ³ cÃ´ng suáº¥t tb) Ä‘áº¡t MAPE=0%, B chá»‰ Ä‘áº¡t 2.7%
- **4 biáº¿n Ä‘á»™c láº­p thá»±c táº¿ cÃ³ giá»›i háº¡n thÃ´ng tin ná»™i táº¡i** â†’ cáº§n thÃªm features má»›i (váº­t lÃ½) hoáº·c má»Ÿ rá»™ng data
- **Optuna tá»‘i Æ°u CV â‰  tá»‘i Æ°u test**: Best CV (279) vs Best Test (287) chÃªnh lá»‡ch Ä‘Ã¡ng ká»ƒ

---

### 3. PHÆ¯Æ NG ÃN B2 â€” Äá»™t PhÃ¡ Vá»›i Deep Learning + Data Augmentation

#### Káº¿t quáº£

- **Best Model:** PT_CNN1D (PyTorch 1D-CNN) on Augmented Data
- **Hiá»‡u suáº¥t:** MAE = 282.93 | MAPE = 2.655% | RÂ² = 0.9290
- **Cáº£i thiá»‡n so vá»›i B:** MAE giáº£m 3.99 (1.39%), MAPE giáº£m 0.043% (1.59%), RÂ² tÄƒng 0.0014

#### Top 10 Models (Overall)

| Rank | Model | Data | MAE | MAPE (%) | RÂ² | nRMSE |
|------|-------|------|-----|----------|-----|-------|
| 1 | **PT_CNN1D** | **Augmented** | **282.93** | **2.655** | **0.9290** | 0.0956 |
| 2 | XGB_500 | Augmented | 284.74 | 2.672 | 0.9283 | 0.0961 |
| 3 | LGBM_500 | Augmented | 284.87 | 2.674 | 0.9284 | 0.0960 |
| 4 | RF_500 | Augmented | 284.98 | 2.675 | 0.9282 | 0.0962 |
| 5 | sklearn_NAS_(256,) | Augmented | 285.10 | 2.679 | 0.9291 | 0.0956 |
| 6 | sklearn_NAS_(16,16) | Original | 286.11 | 2.690 | 0.9287 | 0.0958 |
| 7 | PT_Hybrid_CNN_GRU | Augmented | 286.60 | 2.693 | 0.9288 | 0.0958 |
| 8 | PT_GRU | Augmented | 286.63 | 2.696 | 0.9288 | 0.0957 |
| 9 | sklearn_MLP32 | Augmented | 286.70 | 2.697 | 0.9284 | 0.0960 |
| 10 | PT_LSTM | Augmented | 286.86 | 2.698 | 0.9286 | 0.0959 |

**Baseline reference:** sklearn_MLP32_baseline (Original) â€” MAE=286.92, MAPE=2.698%

#### Data Augmentation

| Ká»¹ thuáº­t | Sá»‘ samples | MÃ´ táº£ |
|----------|-----------|-------|
| Original | 5,220 | Dá»¯ liá»‡u gá»‘c |
| Gaussian Noise | 4,945 | ThÃªm nhiá»…u Gaussian nháº¹ |
| SMOTE Regression | 4,945 | Ná»™i suy giá»¯a cÃ¡c Ä‘iá»ƒm gáº§n nhau |
| Conditional Bootstrap | 4,944 | Bootstrap cÃ³ Ä‘iá»u kiá»‡n theo khoáº£ng |
| Mixup | 4,946 | Trá»™n tuyáº¿n tÃ­nh 2 samples |
| **Tá»”NG** | **25,000** | **TÄƒng 4.79x** |

**LÆ°u Ã½:** Test set luÃ´n lÃ  1,044 samples **gá»‘c** (20% cá»§a 5,220), Ä‘áº£m báº£o Ä‘Ã¡nh giÃ¡ cÃ´ng báº±ng.

#### NAS-Lite Results

NAS-Lite (Neural Architecture Search - Lite) tÃ¬m Ä‘Æ°á»£c cáº¥u trÃºc tá»‘i Æ°u khÃ¡c nhau cho 2 loáº¡i data:

| Dataset | Architecture | Activation | Alpha | Learning Rate | MAE |
|---------|-------------|------------|-------|--------------|-----|
| **Original** | **(16, 16)** | ReLU | 0.1000 | 0.01 | 286.11 |
| **Augmented** | **(256,)** | Tanh | 0.0001 | 0.01 | 285.10 |

**Insight:**
- Data gá»‘c â†’ máº¡ng **sÃ¢u hÆ¡n** (2 lá»›p nhá»), regularization máº¡nh (alpha=0.1)
- Data augmented â†’ máº¡ng **rá»™ng hÆ¡n** (1 lá»›p lá»›n), regularization nháº¹ (alpha=0.0001)
- Augmentation cho phÃ©p máº¡ng há»c complex representations mÃ  khÃ´ng overfit

#### So sÃ¡nh Original vs Augmented

| Model | Original MAE | Augmented MAE | Cáº£i thiá»‡n |
|-------|-------------|---------------|-----------|
| PT_CNN1D | 287.65 | **282.93** | **-4.72** â­ |
| sklearn_NAS | 286.11 | 285.10 | -1.01 |
| sklearn_MLP32 | 286.92 | 286.70 | -0.22 |
| PT_GRU | 287.07 | 286.63 | -0.44 |
| PT_LSTM | 287.36 | 286.86 | -0.50 |
| PT_Hybrid_CNN_GRU | 287.38 | 286.60 | -0.78 |

**Nháº­n xÃ©t:** CNN1D hÆ°á»Ÿng lá»£i **nhiá»u nháº¥t** tá»« augmentation (-4.72 MAE).

#### Nháº­n xÃ©t

âœ… **Æ¯u Ä‘iá»ƒm:**

1. **Data Augmentation hiá»‡u quáº£**
   - Má»Ÿ rá»™ng tá»« 5,220 â†’ 25,000 samples
   - Háº§u háº¿t mÃ´ hÃ¬nh cáº£i thiá»‡n khi dÃ¹ng augmented data
   - **Top 5 Ä‘á»u lÃ  augmented models**

2. **CNN1D lÃ  báº¥t ngá» thÃº vá»‹**
   - VÆ°á»£t trá»™i hÆ¡n GRU/LSTM (mÃ´ hÃ¬nh sequence thÆ°á»ng dÃ¹ng cho time-series)
   - Cho tháº¥y data cÃ³ **local patterns quan trá»ng** trong khÃ´ng gian features
   - Hiá»‡u quáº£ trÃ­ch xuáº¥t interaction giá»¯a cÃ¡c features

3. **Tree-based models cÅ©ng Ä‘Æ°á»£c cáº£i thiá»‡n**
   - XGB_500, LGBM_500, RF_500 Ä‘á»u vÃ o top 4
   - Augmentation giÃºp cÃ¢y quyáº¿t Ä‘á»‹nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n

4. **NAS-Lite tÃ¬m Ä‘Æ°á»£c architecture khÃ¡c biá»‡t**
   - Cho Original vs Augmented data
   - Tá»± Ä‘á»™ng hÃ³a quÃ¡ trÃ¬nh architecture search

âŒ **Háº¡n cháº¿:**

1. **PT_ANN_Advanced phÃ¢n ká»³ nghiÃªm trá»ng trÃªn Original data**
   - MAE = 5,295 (so vá»›i ~287 cá»§a cÃ¡c model khÃ¡c)
   - RÂ² = -19.83 (model tá»‡ hÆ¡n cáº£ dá»± Ä‘oÃ¡n báº±ng mean)
   - Cho tháº¥y deep MLP vá»›i BatchNorm + SELU cÃ³ thá»ƒ khÃ´ng á»•n Ä‘á»‹nh vá»›i data nhá»

2. **Cáº£i thiá»‡n tuyá»‡t Ä‘á»‘i váº«n háº¡n cháº¿**
   - Tá»« 2.698% â†’ 2.655% MAPE (giáº£m 0.043%)
   - MAE giáº£m 3.99 (tá»« 286.92 â†’ 282.93)
   - **Váº«n chÆ°a phÃ¡ vá»¡ hoÃ n toÃ n ceiling 2.5%**

3. **Computational cost cao**
   - Cáº§n GPU, thá»i gian train lÃ¢u hÆ¡n nhiá»u
   - Augmentation + Deep Learning phá»©c táº¡p hÆ¡n baseline

ğŸ”¬ **Insights quan trá»ng:**

1. **Data quantity matters for Deep Learning**
   - Deep models cáº§n nhiá»u data Ä‘á»ƒ trÃ¡nh overfit
   - Augmentation giÃºp cung cáº¥p diversity cho training

2. **CNN1D > GRU/LSTM cho bÃ i toÃ¡n nÃ y**
   - BÃ i toÃ¡n khÃ´ng pháº£i time-series thuáº§n tÃºy
   - Features cÃ³ cáº¥u trÃºc local patterns máº¡nh
   - CNN trÃ­ch xuáº¥t tá»‘t hÆ¡n RNN

3. **Ceiling ~2.65% cÃ³ thá»ƒ lÃ  giá»›i háº¡n lÃ½ thuyáº¿t**
   - Vá»›i chá»‰ 4 biáº¿n Ä‘á»™c láº­p
   - Noise vá»‘n cÃ³ trong quÃ¡ trÃ¬nh Ä‘o Ä‘áº¡c
   - Cáº§n thÃªm features váº­t lÃ½ má»›i Ä‘á»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ

---

## ğŸ” PHÃ‚N TÃCH SÃ‚U: Táº I SAO B1 KHÃ”NG Cáº¢I THIá»†N, NHÆ¯NG B2 CÃ“?

### Váº¥n Ä‘á» cá»§a B1

| KhÃ­a cáº¡nh | Giáº£i thÃ­ch |
|-----------|-----------|
| **Data size** | Váº«n 5,220 samples â€” khÃ´ng Ä‘á»§ cho complex models |
| **Feature space** | Váº«n 4 biáº¿n gá»‘c â†’ thÃ´ng tin ná»™i táº¡i giá»›i háº¡n |
| **Model complexity** | Optuna tÃ¬m Ä‘Æ°á»£c architecture phá»©c táº¡p (26, 71, 38, 119) â†’ risk overfit |
| **Ensemble** | Trung bÃ¬nh cÃ¡c model cÃ¹ng ceiling â†’ khÃ´ng phÃ¡ Ä‘Æ°á»£c tráº§n |

**Káº¿t luáº­n B1:** Khi thÃ´ng tin trong data Ä‘Ã£ bá»‹ khai thÃ¡c tá»‘i Ä‘a, **thÃªm phá»©c táº¡p â‰  cáº£i thiá»‡n**

### Äiá»ƒm Ä‘á»™t phÃ¡ cá»§a B2

| KhÃ­a cáº¡nh | Giáº£i thÃ­ch |
|-----------|-----------|
| **Data augmentation** | 25,000 samples â†’ Deep Learning cÃ³ Ä‘á»§ data Ä‘á»ƒ há»c |
| **CNN architecture** | TrÃ­ch xuáº¥t local patterns mÃ  MLP khÃ´ng náº¯m báº¯t tá»‘t |
| **Inductive bias** | CNN cÃ³ bias phÃ¹ há»£p vá»›i structure cá»§a feature interactions |
| **Regularization implicit** | Augmentation lÃ  regularization tá»± nhiÃªn |

**Káº¿t luáº­n B2:** TÄƒng **data quantity + architecture phÃ¹ há»£p** â†’ phÃ¡ ceiling

---

## ğŸ“Š INSIGHTS Tá»”NG Há»¢P

### 1. Feature Engineering Hierarchy

```
Poly2 (14 feat) > Poly3+Manual (58 feat) > Poly2+Manual (24 feat) > Manual (28 feat) > Poly3 (34 feat)
```

**Takeaway:** 
- Polynomial degree 2 lÃ  sweet spot
- Degree 3 táº¡o quÃ¡ nhiá»u features â†’ overfitting risk
- Manual features khÃ´ng tá»‘t báº±ng polynomial tá»± Ä‘á»™ng

### 2. Model Performance Hierarchy (cho data gá»‘c 5,220 samples)

```
MLP_simple (32) â‰ˆ Ridge â‰¥ Stacking â‰ˆ XGBoost â‰ˆ LightGBM > MLP_deep > SVR
```

**Takeaway:** ÄÆ¡n giáº£n thÆ°á»ng tháº¯ng vá»›i data nhá»

### 3. Data Augmentation Impact

| Model Type | Cáº£i thiá»‡n MAE trung bÃ¬nh |
|------------|--------------------------|
| CNN | -4.72 (1.6%) â­ |
| sklearn NAS | -1.01 (0.4%) |
| GRU | -0.44 (0.2%) |
| LSTM | -0.50 (0.2%) |
| Hybrid CNN-GRU | -0.78 (0.3%) |
| MLP baseline | -0.22 (0.1%) |

**Takeaway:** CNN hÆ°á»Ÿng lá»£i nhiá»u nháº¥t tá»« augmentation

### 4. Giá»›i háº¡n dá»± Ä‘oÃ¡n vá»›i 4 biáº¿n Ä‘á»™c láº­p

| PhÆ°Æ¡ng phÃ¡p | MAPE tá»‘t nháº¥t | Note |
|-------------|--------------|------|
| ML truyá»n thá»‘ng (B, B1) | 2.698% | Ceiling Ä‘Ã£ Ä‘áº¡t |
| Deep Learning + Augmentation (B2) | 2.655% | Cáº£i thiá»‡n nháº¹ |
| **Giá»›i háº¡n lÃ½ thuyáº¿t Æ°á»›c tÃ­nh** | **~2.5-2.6%** | Cáº§n thÃªm features má»›i |

---

## ğŸ¯ Káº¾T LUáº¬N VÃ€ KHUYáº¾N NGHá»Š

### Káº¿t luáº­n chÃ­nh

1. **PhÆ°Æ¡ng Ãn B2 lÃ  winner tá»•ng thá»ƒ**
   - MAE = 282.93, MAPE = 2.655%, RÂ² = 0.9290
   - CNN1D + Data Augmentation phÃ¡ vá»¡ ceiling cá»§a ML truyá»n thá»‘ng

2. **PhÆ°Æ¡ng Ãn B1 cÃ³ giÃ¡ trá»‹ khoa há»c**
   - XÃ¡c nháº­n ceiling cá»§a ML truyá»n thá»‘ng
   - Kháº³ng Ä‘á»‹nh Poly2 + StandardScaler lÃ  optimal cho feature engineering
   - Cho tháº¥y ensemble phá»©c táº¡p khÃ´ng pháº£i lÃºc nÃ o cÅ©ng tá»‘t hÆ¡n

3. **PhÆ°Æ¡ng Ãn B lÃ  baseline vá»¯ng cháº¯c**
   - ÄÆ¡n giáº£n, nhanh, dá»… deploy
   - MLP_(32) lÃ  lá»±a chá»n tá»‘t cho production náº¿u khÃ´ng cáº§n hiá»‡u suáº¥t tá»‘i Ä‘a

### So sÃ¡nh Pros & Cons

| PhÆ°Æ¡ng Ã¡n | Pros | Cons | Use case |
|-----------|------|------|----------|
| **B** | âœ… ÄÆ¡n giáº£n<br>âœ… Nhanh (train <10s)<br>âœ… Dá»… giáº£i thÃ­ch<br>âœ… Stable | âŒ MAPE 2.698% | Production baseline |
| **B1** | âœ… Khoa há»c<br>âœ… XÃ¡c nháº­n ceiling<br>âœ… Feature engineering tá»‘t | âŒ KhÃ´ng cáº£i thiá»‡n<br>âŒ Thá»i gian cao<br>âŒ Phá»©c táº¡p | Research, validation |
| **B2** | âœ… **Hiá»‡u suáº¥t tá»‘t nháº¥t**<br>âœ… PhÃ¡ vá»¡ ceiling<br>âœ… Modern techniques<br>âœ… Scalable vá»›i data | âŒ Cáº§n GPU<br>âŒ Train lÃ¢u<br>âŒ KhÃ³ giáº£i thÃ­ch<br>âŒ Complex pipeline | High-performance production |

### Khuyáº¿n nghá»‹ triá»ƒn khai

#### Scenario 1: Cáº§n deploy nhanh, Ä‘Æ¡n giáº£n
â†’ **Chá»n PhÆ°Æ¡ng Ãn B**
- Model: `MLP_(32)` vá»›i `PolynomialFeatures(degree=2)` + `StandardScaler`
- LÃ½ do: MAPE 2.698% Ä‘Ã£ ráº¥t tá»‘t, train nhanh, dá»… maintain

#### Scenario 2: Cáº§n hiá»‡u suáº¥t tá»‘i Ä‘a, cháº¥p nháº­n phá»©c táº¡p
â†’ **Chá»n PhÆ°Æ¡ng Ãn B2**
- Model: `PT_CNN1D` vá»›i augmented data
- LÃ½ do: MAPE 2.655%, state-of-the-art cho bÃ i toÃ¡n nÃ y
- YÃªu cáº§u: GPU, pipeline augmentation

#### Scenario 3: Research & Development
â†’ **Káº¿t há»£p insight tá»« cáº£ 3**
- Baseline: B (MLP_32)
- Feature engineering: B1 (Poly2 + Standard)
- Advanced: B2 (CNN + Augmentation)

### HÆ°á»›ng cáº£i tiáº¿n tiáº¿p theo

1. **Thu tháº­p thÃªm features váº­t lÃ½**
   - Tá»‘c Ä‘á»™ bÄƒng táº£i (m/s)
   - Táº£i trá»ng váº­t liá»‡u (kg)
   - Äá»™ áº©m váº­t liá»‡u
   - Thá»i gian váº­n hÃ nh liÃªn tá»¥c
   - Tuá»•i bÄƒng táº£i / má»©c Ä‘á»™ mÃ i mÃ²n
   â†’ **CÃ³ thá»ƒ phÃ¡ vá»¡ ceiling 2.5%**

2. **Thá»­ cÃ¡c ká»¹ thuáº­t augmentation khÃ¡c**
   - TimeGAN (náº¿u cÃ³ time-series component)
   - VAE/GAN for tabular data
   - Targeted augmentation (táº­p trung vÃ o vÃ¹ng khÃ³ dá»± Ä‘oÃ¡n)

3. **Ensemble B + B2**
   - MLP_(32) from B (simple, stable)
   - PT_CNN1D from B2 (complex, accurate)
   - Weighted average hoáº·c stacking
   â†’ CÃ¢n báº±ng giá»¯a stability vÃ  accuracy

4. **Uncertainty quantification**
   - Bayesian Neural Networks
   - Dropout at inference
   - Quantile regression
   â†’ Biáº¿t Ä‘Æ°á»£c Ä‘á»™ tin cáº­y cá»§a prediction

5. **Model interpretation**
   - SHAP values cho CNN
   - Feature importance analysis
   - Partial dependence plots
   â†’ Hiá»ƒu Ä‘Æ°á»£c model há»c gÃ¬ tá»« data

---

## ğŸ“ PHá»¤ Lá»¤C

### Cáº¥u trÃºc file káº¿t quáº£

```
ketqua/
â”œâ”€â”€ ket_qua_phuong_an_B.xlsx      # 27 models, Poly2, 5220 samples
â”œâ”€â”€ ket_qua_phuong_an_B1.xlsx     # 12 models, Optuna+Ensemble, 5220 samples
â””â”€â”€ ket_qua_phuong_an_B2.xlsx     # 17 models, Deep Learning, 25000 samples
```

### Sheets trong má»—i file

**ket_qua_phuong_an_B.xlsx:**
1. `Xep hang tong hop` â€” 27 models ranked
2. `Du doan - Best Model` â€” 1044 predictions
3. `Chi so sai so` â€” Error metrics (MAE, MAPE, RÂ², etc.)
4. `Hyperparameter Tuning` â€” XGB, LGBM params

**ket_qua_phuong_an_B1.xlsx:**
1. `Xep hang tong hop` â€” 12 models ranked
2. `Phase1 FeatureSet` â€” 15 feature engineering experiments
3. `Du doan Best B1` â€” 1044 predictions
4. `Chi so sai so` â€” Error metrics
5. `Optuna Best Params` â€” MLP, XGB, LGBM optimized params
6. `So sanh B vs B1` â€” Direct comparison

**ket_qua_phuong_an_B2.xlsx:**
1. `Tong hop` â€” 17 models (Original + Augmented)
2. `Data Goc` â€” 7 models on Original data
3. `Data Augmented` â€” 10 models on Augmented data
4. `Du doan Best` â€” 1044 predictions from PT_CNN1D
5. `NAS-Lite Config` â€” Architecture search results
6. `Augmentation Info` â€” 4 techniques, 25000 total samples

---

## ğŸ TÃ“M Táº®T EXECUTIVE

**Má»¥c tiÃªu:** Dá»± Ä‘oÃ¡n Táº£i tiÃªu thá»¥ bÄƒng táº£i tá»« 4 biáº¿n Ä‘á»™c láº­p thá»±c táº¿

**Káº¿t quáº£:**
- âœ… PhÆ°Æ¡ng Ãn B2 (CNN1D + Augmentation) Ä‘áº¡t **MAPE = 2.655%** â€” tá»‘t nháº¥t
- âœ… Cáº£i thiá»‡n **1.59%** so vá»›i baseline (2.698% â†’ 2.655%)
- âœ… XÃ¡c nháº­n ceiling cá»§a ML truyá»n thá»‘ng á»Ÿ **~2.7%**
- âœ… Data Augmentation + Deep Learning lÃ  chÃ¬a khÃ³a phÃ¡ ceiling

**Khuyáº¿n nghá»‹:**
- ğŸš€ **Production (balanced):** PhÆ°Æ¡ng Ãn B â€” MLP_(32), nhanh, á»•n Ä‘á»‹nh
- ğŸ”¬ **Production (best performance):** PhÆ°Æ¡ng Ãn B2 â€” PT_CNN1D, SOTA
- ğŸ“Š **Äá»ƒ cáº£i thiá»‡n hÆ¡n ná»¯a:** Cáº§n thu tháº­p thÃªm features váº­t lÃ½ má»›i

---

**NgÃ y bÃ¡o cÃ¡o:** 11/02/2026  
**TÃ¡c giáº£:** AI Assistant (Claude Sonnet 4.5)  
**Dá»¯ liá»‡u:** `ketqua/ket_qua_phuong_an_B*.xlsx`
